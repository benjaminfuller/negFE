% Preliminary Definitions and Results for negFE

\section{Preliminaries}
In this section we will introduce existing results to help clarify our place in the literature, provide necessary existing definitions to show what we are borrowing and what we build on, and we will provide new definitions in support of our novel results.

\subsection{Existing Definitions}

\begin{definition}[Entropy]
    \emph{Entropy}, denoted $\ent{X}$, for some random variable $X$ is a measure of how stable the outcomes of the random variable are. It is calculated as \[\ent{X} \defined \sum\limits_{i=1}^n \p{x_i}\log{\p{x_i}}\] where there are $n$ values that $X$ takes and we denote them as $x_i$. 
\end{definition}

\begin{definition}[Min Entropy]
    \emph{Min Entropy}, denoted $\minent{X}$, is a best case measure of the stability of the random variable $X$. It is calculated as \[\minent{X} \defined -\log{\max\limits_{x_i} p(x_i)}\].  
\end{definition}

\begin{definition}[Average Conditional Min Entropy]
    \emph{Average Conditional Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is an average measure of the remaining entropy of the former given the outcome of the latter. It is calclulated as \[ \acminent{X}{Y} \defined -\log{\Exlim{y \leftarrow Y}{\max\limits_{x} \Prob{X = x\ |\ Y = y}}}.\] 
\end{definition}

\begin{definition}[Hartley Entropy]
    \emph{Hartley Entropy} also called \emph{Hartley's Function} measures the uncertainty of a random variable in a basic way, measuring the number of outcomes the random variable has with non-zero probability. It can be computed as 
    \[
    \hart{X} = | \sbr{x \in X\,|\, \Prob{X = x} > 0}|.
    \]
\end{definition}

\begin{definition}[Markov's Inequality]
    Markov's inequality is a tail bound for random variables that gives an upper bound on the probability of a random variable deviating from its mean. Let $X$ be a non-negative valued random variable. Then the following inequality holds for any $\alpha > 0$: 
    \[ 
      \Prob{X \geq \alpha \cdot \Ex{X}} \leq 1/\alpha .
    \]
\end{definition}

\subsection{Previous Results}
It has been shown that universal fuzzy extractors are impossible in the information theoretic setting. 

\subsubsection{Markov Bound for Predictability}
Markov bounds are tail bounds that use Markov's Inequality to bound the probability that a random variable deviates significantly from its expected value. In Markov's inequality, we necessarily lose a multiplicative factor (here called $alpha$) in order to control the probability of the event occuring. When discussing entropy, we are dealing with a log scaled value which makes losing multiplicative factors costly. Instead, we can perform a Markov bound on the predictability scale. In this case, rather than lose a multiplicative factor in entropy, we lose a multiplicative factor in predictability which translates to a small number of bits of entropy lost for the controlled outcomes.

We present the proof here:
Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be independent random variables. Let $Y$ be a random variable arbitrarility correlated with $\vec{X}$. Then we can bound the entropy loss of $\acminent{\vec{X}}{Y}$ using a Markov bound on predictability. 

\begin{align}
    \acminent{\vec{X}}{Y} &= \Delta\\
    -\log{\Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}}} &= \Delta\\
    \Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} &= 2^{-\Delta}\\
    \Problim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y} \geq \alpha \cdot 2^{-\Delta}} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} \geq \log{\alpha} -\Delta} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{-\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{\minent{X\, |\, Y} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha}
\end{align}

\subsection{New Definitions}
Fuzzy extractors with quasipolynomial advice. 