% Preliminary Definitions and Results for negFE

\section{Preliminaries}
In this section we will introduce existing results to help clarify our place in the literature, provide necessary existing definitions to show what we are borrowing and what we build on, and we will provide new definitions in support of our novel results.

\subsection{Existing Definitions}

\begin{definition}[Entropy]
    \emph{Entropy}, denoted $\ent{X}$, for some random variable $X$ is a measure of how stable the outcomes of the random variable are. It is calculated as \[\ent{X} \defined \sum\limits_{i=1}^n \p{x_i}\log{\p{x_i}}\] where there are $n$ values that $X$ takes and we denote them as $x_i$. 
\end{definition}

\begin{definition}[Min Entropy]
    \emph{Min Entropy}, denoted $\minent{X}$, is a best case measure of the stability of the random variable $X$. It is calculated as \[\minent{X} \defined -\log{\max\limits_{x_i} p(x_i)}\].  
\end{definition}

\begin{definition}[Average Conditional Min Entropy]
    \emph{Average Conditional Min Entropy}, denoted $\acminent{X}{Y}$ for two random variables $X$ and $Y$ is an average measure of the remaining entropy of the former given the outcome of the latter. It is calclulated as \[ \acminent{X}{Y} \defined -\log{\Exlim{y \leftarrow Y}{\max\limits_{x} \Prob{X = x\ |\ Y = y}}}.\] 
\end{definition}

\begin{definition}[Hartley Entropy]
    \emph{Hartley Entropy} also called \emph{Hartley's Function} measures the uncertainty of a random variable in a basic way, measuring the number of outcomes the random variable has with non-zero probability. It can be computed as 
    \[
    \hart{X} = | \sbr{x \in X\,|\, \Prob{X = x} > 0}|.
    \]
\end{definition}

\begin{definition}[Markov's Inequality]
    Markov's inequality is a tail bound for random variables that gives an upper bound on the probability of a random variable deviating from its mean. Let $X$ be a non-negative valued random variable. Then the following inequality holds for any $\alpha > 0$: 
    \[ 
      \Prob{X \geq \alpha \cdot \Ex{X}} \leq 1/\alpha .
    \]
\end{definition}

\begin{definition}[Secure Sketch]
    A \emph{secure sketch} is an abstract crytographic primative that hides information of a sample, but allows for recovery of the original sample if presented a sample that is close enough to the original sample. It is made of two algorithms: Sketch and Recover. Sketch takes the original sample and outputs some public value. Recover then takes some other sample and the value from Sketch and if the two samples are close enough, outputs the original sample. 

    Formal Definition here. 
\end{definition}

\subsection{Previous Results}
It has been shown that universal fuzzy extractors are impossible in the information theoretic setting. 

\subsection{Average Conditional Min-Entropy Loss}
\begin{lemma}
    \label{lem:conditionalminentloss}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be independent random variables. 
    Let $Y$ be a random variable arbitrarility correlated with $\vec{X}$. 
    Then 
    \[
        \acminent{\vec{X}}{Y} \geq \sum \minent{X_i} - \hart{Y}
    \]
\end{lemma} 

\begin{proof}
    Since each $X_i$ is independent then $\minent{\vec{X}} = \sum \minent{X_i}$.
    Now, by definition,
    \begin{align}
        \acminent{\vec{X}}{Y} &= -\log{\Exlim{y \leftarrow Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x}\ |\ Y = y}}} \\
        &= -log{\sum\limits_{y} \max\limits_{\vec{x}} \Prob{\vec{X} = \vec{x} \ |\ Y = y} \cdot \Prob{Y = y}}\\
        &= -log{\sum\limits_{y} \max\limits_{\vec{x}} \Prob{\vec{X} = \vec{x} \vee Y = y}}\\
        &\geq -log{\sum\limits_{y} \max\limits_{\vec{x}, y'} \Prob{\vec{X} = \vec{x}  ^ Y = y'}}\\
        &= -\log{2^{\hart{Y}} \cdot 2^{\minent{\vec{X},Y}}}\\
        &= \minent{\vec{X},Y} - \hart{Y}\\
        &\geq \minent{\vec{X}} - \hart{Y} \\   
        &= \sum \minent{X_i} - \hart{Y}
    \end{align}
\end{proof}

\subsubsection{Markov Bound for Predictability}
Markov bounds are tail bounds that use Markov's Inequality to bound the probability that a random variable deviates significantly from its expected value. In Markov's inequality, we necessarily lose a multiplicative factor (here called $alpha$) in order to control the probability of the event occuring. When discussing entropy, we are dealing with a log scaled value which makes losing multiplicative factors costly. Instead, we can perform a Markov bound on the predictability scale. In this case, rather than lose a multiplicative factor in entropy, we lose a multiplicative factor in predictability which translates to a small number of bits of entropy lost for the controlled outcomes.

\begin{lemma}
    \label{lem:markovpred}
    Let $\vec{X} = (X_1, X_2, \ldots, X_k)$ be independent random variables. Let $Y$ be a random variable arbitrarility correlated with $\vec{X}$. 
    Let $\alpha > 0$, then for all but a $(1-1/\alpha)$ fraction of the $X_i$ the entropy loss is less than $\log{\alpha}/k$
\end{lemma}

\begin{proof} 

\begin{align}
    \acminent{\vec{X}}{Y} &= \Delta\\
    -\log{\Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}}} &= \Delta\\
    \Exlim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} &= 2^{-\Delta}\\
    \Problim{Y}{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y} \geq \alpha \cdot 2^{-\Delta}} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} \geq \log{\alpha} -\Delta} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{-\log{\max\limits_{\vec{x}} \Prob{\Vec{X} = \vec{x} \,|\, Y = y}} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha} \\
    \Problim{Y}{\minent{\vec{X}\, |\, Y} < \Delta -\log{\alpha}} &\leq \frac{1}{\alpha}
\end{align}
    
\end{proof}

\subsubsection{Upper bound for size of a Fuzzy Extractor}
In \cite{fuller2020fuzzy}, Fuller et al. show that the size of a fuzzy extractor can be upper bound in a general case. 
We restate their lemma here for completeness of our main proof. 

\begin{lemma}[Lemma 5.2 in \cite{fuller2020fuzzy}]
    \label{lem:smallgeneralviable}
    Suppose $\mathcal{M}$ is $\zeroone{n}$ with the Hamming Metric, $\kappa \geq 2$, $0 \leq t \leq n/2$, and $\epsilon > 0$. 
    Suppose $(\mathsf{Gen, Rep})$ is a $(\mathcal{M,W},\kappa, t, \epsilon)$-fuzzy extractor with error $\delta = 0$, for some distribution family $\mathcal{W}$ over $\mathcal{M}$. 
    Let $\tau = t/n$. 
    For any fixed $p$, there is a set $\mathsf{GoodKey}_p \subseteq \zeroone{\kappa}$ of size at least $2^{\kappa - 1}$ such that for every $\mathsf{key} \in \mathsf{GoodKey}_p$,
    \[
        \log{|\sbr{v \in \mathcal{M}|\prns{\mathsf{key}, p} \in \mathsf{supp}\prns{\mathsf{Gen}(v)}}|} \leq n \cdot h_2\prns{\frac{1}{2} - \tau} \leq n \cdot \prns{1 - \frac{2}{\ln{2}} \cdot \tau^{2}}, 
    \]   
    and, therefore, for any distribution $D_{\mathcal{M}}$ on $\mathcal{M}$, 
    \[
        \hart{D_{\mathcal{M}}|\mathsf{Gen}\prns{D_{\mathcal{M}}} = \prns{\mathsf{key}, p}} \leq n \cdot h_2\prns{\frac{1}{2} - \tau} \leq n \cdot \prns{1 - \frac{2}{\ln{2}} \cdot \tau^{2}}.
    \]   
\end{lemma}

\subsection{New Definitions}
Fuzzy extractors with quasipolynomial advice. 